{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Парсинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Лента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "\n",
    "import aiohttp\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WORKER_COUNT = 20\n",
    "\n",
    "def get_cards_urls(url):\n",
    "    time.sleep(0.2)\n",
    "    res = requests.get(url)\n",
    "    page = BeautifulSoup(res.text)\n",
    "    cards = page.find_all(\"a\", {\"class\": \"card-full-news\"})\n",
    "    cards_urls = [x.attrs[\"href\"] for x in cards]\n",
    "    cards_urls = [f\"https://lenta.ru/{x}\" for x in cards_urls]\n",
    "    return cards_urls\n",
    "\n",
    "\n",
    "def get_all_cards_urls(date):\n",
    "    page_num = 1\n",
    "    cards_urls = []\n",
    "    while True:\n",
    "        month = date.month if date.month >= 10 else \"0\" + str(date.month)\n",
    "        day = date.day if date.day >= 10 else \"0\" + str(date.day)\n",
    "        url = f\"https://lenta.ru/news/{date.year}/{month}/{day}/page/{page_num}\"\n",
    "        print(url)\n",
    "        try:\n",
    "            urls = get_cards_urls(url)\n",
    "            if not urls:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        cards_urls.extend(get_cards_urls(url))\n",
    "        page_num += 1\n",
    "\n",
    "    return cards_urls\n",
    "\n",
    "\n",
    "async def save_page(url):\n",
    "    print(\"save page:\", url)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            text = await response.text()\n",
    "\n",
    "    page = BeautifulSoup(text)\n",
    "    topic = page.find(\"a\", {\"class\": \"topic-header__rubric\"}).text\n",
    "    header = page.find(\"span\", {\"class\": \"topic-body__title\"}).text\n",
    "    text = \" \".join([x.text for x in page.find_all(\"p\", {\"class\": \"topic-body__content-text\"})])\n",
    "    filename = f\"./lentaru/{time.time()}.txt\"\n",
    "    file = open(filename, \"w\", encoding=\"utf8\")\n",
    "    file.write(f\"{url}\\n{topic}\\n{header}\\n{text}\")\n",
    "    file.close()\n",
    "\n",
    "\n",
    "async def url_getter(queue):\n",
    "    d = date(year=2023, month=4, day=4)\n",
    "    while True:\n",
    "        print(d)\n",
    "        try:\n",
    "            urls = get_all_cards_urls(d)\n",
    "        except:\n",
    "            pass\n",
    "        [queue.put_nowait(x) for x in urls]\n",
    "        d -= timedelta(days=1)\n",
    "        await queue.join()\n",
    "\n",
    "\n",
    "async def text_getter(queue):\n",
    "    while True:\n",
    "        url = await queue.get()\n",
    "        try:\n",
    "            await save_page(url)\n",
    "        except:\n",
    "            pass\n",
    "        queue.task_done()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    queue = asyncio.Queue()\n",
    "    await asyncio.gather(\n",
    "        url_getter(queue),\n",
    "        *[text_getter(queue) for x in range(WORKER_COUNT)]\n",
    "    )\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фонтанка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "date = datetime.date(year=2023, month=12, day=31)\n",
    "\n",
    "\n",
    "while True:\n",
    "    date_str = date.strftime(\"%d.%m.%Y\")\n",
    "    page = 0\n",
    "    page_urls = []\n",
    "    print(\"get page urls\")\n",
    "    while True:\n",
    "        page += 1\n",
    "        url = f\"https://newsapi.fontanka.ru/v1/public/fontanka/services/archive/?regionId=478&page={page}&pagesize=20&date={date_str}&rubricId=all\"\n",
    "        print(url)\n",
    "        res = requests.get(url)\n",
    "\n",
    "        if res.json()[\"data\"] is None:\n",
    "            break\n",
    "\n",
    "        for item in res.json()[\"data\"]:\n",
    "            if \"https://www.fontanka.ru/\" in item[\"urls\"][\"urlCanonical\"]:\n",
    "                rubrics = [x[\"name\"] for x in item[\"rubrics\"]]\n",
    "                page_urls.append((item[\"urls\"][\"urlCanonical\"], rubrics))\n",
    "\n",
    "    date -= datetime.timedelta(days=1)\n",
    "\n",
    "    data = []\n",
    "    for url, topics in page_urls:\n",
    "        print(\"get text:\", url)\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "            page = BeautifulSoup(res.text)\n",
    "            ps = page.find(\"section\", {\"itemprop\": \"articleBody\"}).find_all(\"p\")\n",
    "            text = \"\".join([x.text for x in ps])\n",
    "            data.append((text, topics))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    file = open(f\"./data/{date_str}.txt\", 'w')\n",
    "    file.write(json.dumps(data))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных и обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Комиссия по вопросам топонимики и охраны истор...</td>\n",
       "      <td>Бывший СССР</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Компания Apple может выпустить iPhone 8 в 2018...</td>\n",
       "      <td>Наука и техника</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>В Петербурге сотрудники ОМОН «Бастион» Росгвар...</td>\n",
       "      <td>Силовые структуры</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Группа английских ученых из Imperial College и...</td>\n",
       "      <td>Наука и техника</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Вечером в пятницу российские миротворцы в Южно...</td>\n",
       "      <td>Бывший СССР</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100953</th>\n",
       "      <td>Россия находится на первом месте в мире по чис...</td>\n",
       "      <td>Путешествия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100954</th>\n",
       "      <td>Вратарь «Ботафого» и сборной Бразилии Жефферсо...</td>\n",
       "      <td>Спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100955</th>\n",
       "      <td>Президент России Владимир Путин в пятницу, 4 н...</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100956</th>\n",
       "      <td>Акции американской компании Virgin Galactic пр...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100957</th>\n",
       "      <td>Президент Белоруссии Александр Лукашенко рассе...</td>\n",
       "      <td>Экономика</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100958 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text              topic\n",
       "0       Комиссия по вопросам топонимики и охраны истор...        Бывший СССР\n",
       "1       Компания Apple может выпустить iPhone 8 в 2018...    Наука и техника\n",
       "2       В Петербурге сотрудники ОМОН «Бастион» Росгвар...  Силовые структуры\n",
       "3       Группа английских ученых из Imperial College и...    Наука и техника\n",
       "4       Вечером в пятницу российские миротворцы в Южно...        Бывший СССР\n",
       "...                                                   ...                ...\n",
       "100953  Россия находится на первом месте в мире по чис...        Путешествия\n",
       "100954  Вратарь «Ботафого» и сборной Бразилии Жефферсо...              Спорт\n",
       "100955  Президент России Владимир Путин в пятницу, 4 н...             Россия\n",
       "100956  Акции американской компании Virgin Galactic пр...          Экономика\n",
       "100957  Президент Белоруссии Александр Лукашенко рассе...          Экономика\n",
       "\n",
       "[100958 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = data.copy()\n",
    "\n",
    "X.loc[X[\"topic\"] == \"Россия\", \"topic\"] = 0\n",
    "X.loc[X[\"topic\"] == \"Общество\", \"topic\"] = 0\n",
    "X.loc[X[\"topic\"] == \"Экономика\", \"topic\"] = 1\n",
    "X.loc[X[\"topic\"] == \"Силовые структуры\", \"topic\"] = 2\n",
    "X.loc[X[\"topic\"] == \"Бывший СССР\", \"topic\"] = 3\n",
    "X.loc[X[\"topic\"] == \"Спорт\", \"topic\"] = 4\n",
    "X.loc[X[\"topic\"] == \"Забота о себе\", \"topic\"] = 5\n",
    "X.loc[X[\"topic\"] == \"Строительство\", \"topic\"] = 6\n",
    "X.loc[X[\"topic\"] == \"Путешествия\", \"topic\"] = 7\n",
    "X.loc[X[\"topic\"] == \"Туризм\", \"topic\"] = 7\n",
    "X.loc[X[\"topic\"] == \"Наука и техника\", \"topic\"] = 8\n",
    "\n",
    "X = shuffle(X)\n",
    "Y = X[\"topic\"]\n",
    "X = X[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/andreyserov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/andreyserov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 100958/100958 [01:51<00:00, 904.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "russian_stop_words = stopwords.words(\"russian\")\n",
    "noise = list(punctuation) + russian_stop_words + [\"«\", \"»\"]\n",
    "\n",
    "res = []\n",
    "for v in tqdm(list(X)):\n",
    "    r = \" \".join([x for x in word_tokenize(v) if x not in noise])\n",
    "    res.append(r)\n",
    "X = pd.DataFrame(res)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 100958/100958 [04:51<00:00, 346.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "res = []\n",
    "for v in tqdm(list(X)):\n",
    "    r = m.lemmatize(v)\n",
    "    res.append(\"\".join(r))\n",
    "X = pd.DataFrame(res)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90862, 90862, 10096, 10096)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<90862x20860 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 11758462 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(smooth_idf=True, min_df=0.001, ngram_range=(1, 4))\n",
    "\n",
    "x_train_tdidf = tfidf_vec.fit_transform(x_train)\n",
    "x_test_tdidf = tfidf_vec.transform(x_test)\n",
    "x_train_tdidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.layers import BatchNormalization, Activation, Dropout\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "model = Sequential()\n",
    "for _ in range(10):\n",
    "    model.add(Dense(400, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(9, activation=\"softmax\"))\n",
    "\n",
    "optimizer = Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2840/2840 [==============================] - 257s 81ms/step - loss: 0.5597 - accuracy: 0.8140 - val_loss: 0.3468 - val_accuracy: 0.8774\n",
      "Epoch 2/5\n",
      "2840/2840 [==============================] - 219s 77ms/step - loss: 0.3186 - accuracy: 0.8943 - val_loss: 0.3552 - val_accuracy: 0.8878\n",
      "Epoch 3/5\n",
      "2840/2840 [==============================] - 220s 78ms/step - loss: 0.2445 - accuracy: 0.9201 - val_loss: 0.3550 - val_accuracy: 0.8910\n",
      "Epoch 4/5\n",
      "2840/2840 [==============================] - 218s 77ms/step - loss: 0.1659 - accuracy: 0.9472 - val_loss: 0.3927 - val_accuracy: 0.8873\n",
      "Epoch 5/5\n",
      "2840/2840 [==============================] - 220s 78ms/step - loss: 0.1090 - accuracy: 0.9661 - val_loss: 0.4595 - val_accuracy: 0.8885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9e68233a90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train_tdidf.toarray(), y_train.astype(np.int32),\n",
    "    epochs=5,\n",
    "    validation_data=(x_test_tdidf.toarray(), y_test.astype(np.int32)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
